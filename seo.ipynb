{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9dCJFkXPd3e",
        "outputId": "313b28b1-8883-4f10-8505-c9fbcf75dcef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'seo · GitHub Topics · GitHub', 'meta_description': 'GitHub is where people build software. More than 100 million people use GitHub to discover, fork, and contribute to over 420 million projects.'}\n",
            "SEO Score for https://github.com/topics/seo: 89.11/100\n",
            "Ping to github.com: 8.35 ms\n",
            "Resolved IP addresses for github.com: ['140.82.116.4']\n",
            "DNS Response Time for github.com: 0.0030 seconds\n",
            "HTTPS Enabled: Yes\n",
            "size of the content 478.599609375kb\n",
            "Time to First Byte (TTFB): 0.0339 seconds\n",
            "Analyzing accessibility for https://github.com/topics/seo...\n",
            "\n",
            "Images with alt text: 3/3\n",
            "Inputs with labels: 4/11\n",
            "Heading structure: ['h1: 4', 'h2: 5', 'h3: 21', 'h4: 0', 'h5: 0', 'h6: 0']\n",
            "ARIA-labeled elements: 88\n",
            "\n",
            "Accessibility Score: 100.00/100\n",
            "Checking broken links for https://github.com/topics/seo...\n",
            "\n",
            "Broken links found: 12\n",
            "https://github.com/vuesion/vuesion/pulls\n",
            "https://github.com/topics/best-practises\n",
            "https://github.com/kpumuk/meta-tags/issues\n",
            "https://github.com/kpumuk/meta-tags/pulls\n",
            "https://github.com/Atyantik/react-pwa/issues\n",
            "https://github.com/Atyantik/react-pwa/pulls\n",
            "https://github.com/marcobiedermann/search-engine-optimization/issues\n",
            "https://github.com/marcobiedermann/search-engine-optimization/pulls\n",
            "https://github.com/spatie/laravel-sitemap/issues\n",
            "https://github.com/spatie/laravel-sitemap/pulls\n",
            "https://github.com/madawei2699/awesome-seo/issues\n",
            "https://github.com/madawei2699/awesome-seo/pulls\n",
            "\n",
            "Broken links summary:\n",
            "Broken link: https://github.com/vuesion/vuesion/pulls\n",
            "Broken link: https://github.com/topics/best-practises\n",
            "Broken link: https://github.com/kpumuk/meta-tags/issues\n",
            "Broken link: https://github.com/kpumuk/meta-tags/pulls\n",
            "Broken link: https://github.com/Atyantik/react-pwa/issues\n",
            "Broken link: https://github.com/Atyantik/react-pwa/pulls\n",
            "Broken link: https://github.com/marcobiedermann/search-engine-optimization/issues\n",
            "Broken link: https://github.com/marcobiedermann/search-engine-optimization/pulls\n",
            "Broken link: https://github.com/spatie/laravel-sitemap/issues\n",
            "Broken link: https://github.com/spatie/laravel-sitemap/pulls\n",
            "Broken link: https://github.com/madawei2699/awesome-seo/issues\n",
            "Broken link: https://github.com/madawei2699/awesome-seo/pulls\n",
            "No broken links found.\n",
            "Checking schema markup for https://github.com/topics/seo...\n",
            "\n",
            "No Schema Markup found\n",
            "Most Common Keywords:\n",
            "Enter the repetition threshold (default 10): \n",
            "Enter the phrase length (default 1 for single words): 2\n",
            "Repeated phrases: ['issues pull', 'pull requests', 'search engine', 'code issues']\n",
            "Content duplication percentage: 0.30%\n",
            "Final content duplication percentage: 0.30%\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def get_page_load_time(url):\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}#a t\n",
        "        response = requests.get(url, headers=headers)\n",
        "        load_time = time.time() - start_time\n",
        "        return load_time if response.status_code == 200 else None\n",
        "    except requests.exceptions.RequestException:\n",
        "        return None\n",
        "\n",
        "def check_https(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    return parsed_url.scheme == 'https'\n",
        "\n",
        "def get_meta_data(url):\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}#added this\n",
        "\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Check for title and description\n",
        "        title = soup.title.string if soup.title else ''\n",
        "        description = soup.find('meta', attrs={'name': 'description'})\n",
        "        description = description['content'] if description else ''\n",
        "\n",
        "        return title, description\n",
        "    except requests.exceptions.RequestException:\n",
        "        return '', ''\n",
        "\n",
        "def check_image_optimization(url):\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}#a t\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        images = soup.find_all('img')\n",
        "\n",
        "        optimized_images = sum(1 for img in images if img.has_attr('alt') and img['alt'])\n",
        "        total_images = len(images)\n",
        "        return optimized_images / total_images if total_images > 0 else 1\n",
        "    except requests.exceptions.RequestException:\n",
        "        return 0\n",
        "\n",
        "def calculate_seo_score(url):\n",
        "    # Parameters and their weightage\n",
        "    weights = {\n",
        "        'load_time': 20,\n",
        "        'https': 20,\n",
        "        'meta_tags': 20,\n",
        "        'image_optimization': 20,\n",
        "        'mobile_friendly': 20\n",
        "    }\n",
        "\n",
        "\n",
        "    load_time = get_page_load_time(url)\n",
        "    load_time_score = max(0, (2 - load_time) / 2) if load_time else 0\n",
        "\n",
        "\n",
        "    https_score = 1 if check_https(url) else 0\n",
        "\n",
        "\n",
        "    title, description = get_meta_data(url)\n",
        "    meta_tags_score = 1 if title and description else 0\n",
        "\n",
        "    image_optimization_score = check_image_optimization(url)\n",
        "\n",
        "\n",
        "    mobile_friendly_score = 0.9  # Replace with actual score from PageSpeed Insights API\n",
        "\n",
        "    total_score = (\n",
        "        load_time_score * weights['load_time'] +\n",
        "        https_score * weights['https'] +\n",
        "        meta_tags_score * weights['meta_tags'] +\n",
        "        image_optimization_score * weights['image_optimization'] +\n",
        "        mobile_friendly_score * weights['mobile_friendly']\n",
        "    )\n",
        "\n",
        "    max_score = sum(weights.values())  # Maximum possible score\n",
        "    seo_score = (total_score / max_score) * 100  # Convert to percentage\n",
        "\n",
        "    print(f\"SEO Score for {url}: {seo_score:.2f}/100\")\n",
        "    return seo_score\n",
        "\n",
        "def get_website_data(url):\n",
        "    try:\n",
        "        # Fetch the page content\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}#a t\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise an error for bad responses\n",
        "\n",
        "        # Parse the HTML\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Get title\n",
        "        title = soup.title.string if soup.title else 'Title not found'\n",
        "\n",
        "        # Get meta description\n",
        "        meta_desc = soup.find('meta', attrs={'name': 'description'})\n",
        "        description = meta_desc['content'] if meta_desc and meta_desc.has_attr('content') else 'Meta description not found'\n",
        "\n",
        "        # Return extracted values\n",
        "        return {\n",
        "            'title': title,\n",
        "            'meta_description': description,\n",
        "        }\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching data from {url}: {e}\")\n",
        "        return {}\n",
        "from ping3 import ping\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def get_ping(url):\n",
        "    try:\n",
        "        # Extract hostname from URL (remove http/https)\n",
        "        parsed_url = urlparse(url)\n",
        "        hostname = parsed_url.hostname\n",
        "\n",
        "        if not hostname:\n",
        "            print(\"Invalid URL\")\n",
        "            return None\n",
        "\n",
        "        # Ping the hostname\n",
        "        response_time = ping(hostname)  # Returns time in seconds or None if the host is unreachable\n",
        "\n",
        "        if response_time is None:\n",
        "            print(f\"Failed to ping {hostname}\")\n",
        "            return None\n",
        "        else:\n",
        "            # Convert response time to milliseconds\n",
        "            response_time_ms = response_time * 1000\n",
        "            print(f\"Ping to {hostname}: {response_time_ms:.2f} ms\")\n",
        "            return response_time_ms\n",
        "    except Exception as e:\n",
        "        print(f\"Error pinging {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "import dns.resolver\n",
        "import time\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def get_dns_response_time(url):\n",
        "    # Extract the hostname from the URL\n",
        "    parsed_url = urlparse(url)\n",
        "    hostname = parsed_url.hostname\n",
        "\n",
        "    if not hostname:\n",
        "        print(\"Invalid URL\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Start measuring time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Perform DNS resolution\n",
        "        answers = dns.resolver.resolve(hostname, 'A')  # 'A' record for IPv4 addresses\n",
        "\n",
        "        # End measuring time\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Calculate response time\n",
        "        dns_response_time = end_time - start_time\n",
        "\n",
        "        # Print the resolved IP addresses\n",
        "        resolved_ips = [answer.to_text() for answer in answers]\n",
        "        print(f\"Resolved IP addresses for {hostname}: {resolved_ips}\")\n",
        "        print(f\"DNS Response Time for {hostname}: {dns_response_time:.4f} seconds\")\n",
        "\n",
        "        return dns_response_time\n",
        "    except Exception as e:\n",
        "        print(f\"Error resolving {hostname}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def get_ttfb(url):\n",
        "    session = requests.Session()\n",
        "    request = requests.Request(\"GET\", url)\n",
        "    prepared_request = session.prepare_request(request)\n",
        "    start = time.time()\n",
        "\n",
        "    # Only receive the headers to measure TTFB\n",
        "    response = session.send(prepared_request, stream=True)\n",
        "    ttfb = time.time() - start\n",
        "    return ttfb if response.status_code == 200 else None\n",
        "\n",
        "\n",
        "\n",
        "def check_https(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    return parsed_url.scheme == 'https'\n",
        "\n",
        "def get_content_size(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return len(response.content) / 1024  # Return size in KB\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#accessbility\n",
        "\n",
        "\n",
        "\n",
        "def check_alt_attributes(soup):\n",
        "    images = soup.find_all('img')\n",
        "    images_with_alt = [img for img in images if img.has_attr('alt') and img['alt'].strip()]\n",
        "    return len(images_with_alt), len(images)\n",
        "\n",
        "def check_form_labels(soup):\n",
        "    forms = soup.find_all('form')\n",
        "    labels = soup.find_all('label')\n",
        "    inputs_with_labels = sum(1 for label in labels if label.get('for'))\n",
        "    total_inputs = len(soup.find_all(['input', 'select', 'textarea']))\n",
        "    return inputs_with_labels, total_inputs\n",
        "\n",
        "def check_headings_structure(soup):\n",
        "    headings = [soup.find_all(f\"h{i}\") for i in range(1, 7)]\n",
        "    heading_levels = [len(h) for h in headings]\n",
        "    return heading_levels\n",
        "\n",
        "def check_aria_attributes(soup):\n",
        "    aria_elements = soup.find_all(attrs={\"aria-label\": True})\n",
        "    return len(aria_elements)\n",
        "\n",
        "def analyze_accessibility(url):\n",
        "    print(f\"Analyzing accessibility for {url}...\\n\")\n",
        "\n",
        "    # Request the page content\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Failed to load the page.\")\n",
        "        return None\n",
        "\n",
        "    # Parse the HTML\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # 1. Check alt attributes in images\n",
        "    alt_ok, total_images = check_alt_attributes(soup)\n",
        "    print(f\"Images with alt text: {alt_ok}/{total_images}\")\n",
        "\n",
        "    # 2. Check form labels\n",
        "    labels_ok, total_inputs = check_form_labels(soup)\n",
        "    print(f\"Inputs with labels: {labels_ok}/{total_inputs}\")\n",
        "\n",
        "    # 3. Check heading structure (ensuring h1 exists and structure flows)\n",
        "    heading_levels = check_headings_structure(soup)\n",
        "    print(f\"Heading structure: {[f'h{i+1}: {heading_levels[i]}' for i in range(6)]}\")\n",
        "\n",
        "    # 4. Check ARIA attributes\n",
        "    aria_count = check_aria_attributes(soup)\n",
        "    print(f\"ARIA-labeled elements: {aria_count}\")\n",
        "\n",
        "    # Calculate accessibility score\n",
        "    accessibility_score = (\n",
        "        (alt_ok / total_images if total_images > 0 else 1) * 25 +\n",
        "        (labels_ok / total_inputs if total_inputs > 0 else 1) * 25 +\n",
        "        (1 if heading_levels[0] > 0 else 0) * 25 +  # H1 check\n",
        "        (aria_count / 10 if aria_count > 0 else 0) * 25\n",
        "    )\n",
        "    #minor change\n",
        "    if accessibility_score > 100:\n",
        "        accessibility_score = 100\n",
        "    print(f\"\\nAccessibility Score: {accessibility_score:.2f}/100\")\n",
        "\n",
        "url = \"https://github.com/topics/seo\"\n",
        "print(get_website_data(url))\n",
        "calculate_seo_score(url)\n",
        "get_ping(url)\n",
        "get_dns_response_time(url)\n",
        "https_check = check_https(url)\n",
        "print(f\"HTTPS Enabled: {'Yes' if https_check else 'No'}\")\n",
        "print(f\"size of the content {get_content_size(url)}kb\")\n",
        "ttfb = get_ttfb(url)\n",
        "print(f\"Time to First Byte (TTFB): {ttfb:.4f} seconds\" if ttfb else \"Failed to retrieve TTFB.\")\n",
        "analyze_accessibility(url)\n",
        "\n",
        "#additions\n",
        "\n",
        "#1\n",
        "import requests\n",
        "from urllib.parse import urljoin\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def check_link(url, link):\n",
        "    full_link = urljoin(url, link)\n",
        "    try:\n",
        "        res = requests.head(full_link, timeout=5)\n",
        "        if res.status_code >= 400:\n",
        "            return full_link\n",
        "    except requests.exceptions.RequestException:\n",
        "        return full_link\n",
        "    return None\n",
        "\n",
        "def check_broken_links(url):\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        links = [a.get('href') for a in soup.find_all('a', href=True)]\n",
        "\n",
        "        broken_links = []\n",
        "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "            results = executor.map(lambda link: check_link(url, link), links)\n",
        "            broken_links = [link for link in results if link]\n",
        "\n",
        "        print(f\"Broken links found: {len(broken_links)}\")\n",
        "        for bl in broken_links:\n",
        "            print(bl)\n",
        "        return broken_links\n",
        "    except requests.exceptions.RequestException:\n",
        "        print(\"Failed to retrieve page content.\")\n",
        "        return []\n",
        "print(f\"Checking broken links for {url}...\\n\")\n",
        "\n",
        "broken_links = check_broken_links(url)\n",
        "\n",
        "if broken_links:\n",
        "    print(\"\\nBroken links summary:\")\n",
        "    for link in broken_links:\n",
        "        print(f\"Broken link: {link}\")\n",
        "    else:\n",
        "        print(\"No broken links found.\")\n",
        "\n",
        "#2\n",
        "def check_schema_markup(url):\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        schema_data = soup.find_all('script', attrs={'type': 'application/ld+json'})\n",
        "\n",
        "        if schema_data:\n",
        "            print(\"Schema Markup Found\")\n",
        "            for schema in schema_data:\n",
        "                print(schema.text)\n",
        "        else:\n",
        "            print(\"No Schema Markup found\")\n",
        "    except requests.exceptions.RequestException:\n",
        "        print(\"Failed to retrieve page content.\")\n",
        "print(f\"Checking schema markup for {url}...\\n\")\n",
        "check_schema_markup(url)\n",
        "\n",
        "#3\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def keyword_analysis(url):\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        text = soup.get_text().lower()\n",
        "        words = re.findall(r'\\b\\w+\\b', text)\n",
        "        common_words = Counter(words).most_common(10)\n",
        "\n",
        "        print(\"Most Common Keywords:\")\n",
        "        return common_words\n",
        "    except requests.exceptions.RequestException:\n",
        "        print(\"Failed to retrieve page content.\")\n",
        "keyword_analysis(url)\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def check_content_duplication(url, threshold=10, phrase_length=1):\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        text = soup.get_text().lower()\n",
        "\n",
        "        # Create phrases based on the specified phrase length\n",
        "        words = re.findall(r'\\b\\w+\\b', text)\n",
        "        phrases = [' '.join(words[i:i + phrase_length]) for i in range(len(words) - phrase_length + 1)]\n",
        "        word_count = len(phrases)\n",
        "\n",
        "        repeated_phrases = [phrase for phrase, count in Counter(phrases).items() if count > threshold]\n",
        "        print(f\"Repeated phrases: {repeated_phrases}\")\n",
        "\n",
        "        duplication_percentage = (len(repeated_phrases) / word_count) * 100 if word_count > 0 else 0\n",
        "        print(f\"Content duplication percentage: {duplication_percentage:.2f}%\")\n",
        "        return duplication_percentage\n",
        "    except requests.exceptions.RequestException:\n",
        "        print(\"Failed to retrieve page content.\")\n",
        "\n",
        "threshold = int(input(\"Enter the repetition threshold (default 10): \") or 10)\n",
        "phrase_length = int(input(\"Enter the phrase length (default 1 for single words): \") or 1)\n",
        "\n",
        "duplication_percentage = check_content_duplication(url, threshold, phrase_length)\n",
        "print(f\"Final content duplication percentage: {duplication_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ping3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya5TqITVPstq",
        "outputId": "e3ac9763-af0b-4944-e908-2f460d24635f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ping3 in /usr/local/lib/python3.10/dist-packages (4.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dnspython\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOCC8SauPyMj",
        "outputId": "56d653d4-a741-4c3b-9300-7606ea715015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dnspython in /usr/local/lib/python3.10/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://colab.research.google.com/drive/1F_-mWLrNWXrRQIWjs40M9Qpg5v-h3CxC#scrollTo=61tll6yVWiu8\"\n",
        "\n",
        "# Get website data\n",
        "website_data = get_website_data(url)\n",
        "print(website_data)\n",
        "\n",
        "# Calculate SEO score\n",
        "seo_score = calculate_seo_score(url)\n",
        "\n",
        "# Check ping time\n",
        "ping_time = get_ping(url)\n",
        "\n",
        "# Check DNS response time\n",
        "dns_time = get_dns_response_time(url)\n",
        "\n",
        "# Check HTTPS\n",
        "https_check = check_https(url)\n",
        "\n",
        "# Get content size\n",
        "content_size = get_content_size(url)\n",
        "\n",
        "# Check Time to First Byte (TTFB)\n",
        "ttfb = get_ttfb(url)\n",
        "\n",
        "# Analyze accessibility\n",
        "analyze_accessibility(url)\n",
        "\n",
        "# Check broken links\n",
        "broken_links = check_broken_links(url)\n",
        "\n",
        "# Check schema markup\n",
        "check_schema_markup(url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSfIwf_eQE7u",
        "outputId": "73c47b89-f6ba-41ba-f4bc-e81220989b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'Google Colab', 'meta_description': 'Meta description not found'}\n",
            "SEO Score for https://colab.research.google.com/drive/1F_-mWLrNWXrRQIWjs40M9Qpg5v-h3CxC#scrollTo=61tll6yVWiu8: 76.86/100\n",
            "Ping to colab.research.google.com: 1.72 ms\n",
            "Resolved IP addresses for colab.research.google.com: ['74.125.135.138', '74.125.135.100', '74.125.135.102', '74.125.135.101', '74.125.135.139', '74.125.135.113']\n",
            "DNS Response Time for colab.research.google.com: 0.0035 seconds\n",
            "Analyzing accessibility for https://colab.research.google.com/drive/1F_-mWLrNWXrRQIWjs40M9Qpg5v-h3CxC#scrollTo=61tll6yVWiu8...\n",
            "\n",
            "Images with alt text: 0/0\n",
            "Inputs with labels: 0/0\n",
            "Heading structure: ['h1: 0', 'h2: 0', 'h3: 0', 'h4: 0', 'h5: 0', 'h6: 0']\n",
            "ARIA-labeled elements: 1\n",
            "\n",
            "Accessibility Score: 52.50/100\n",
            "Broken links found: 0\n",
            "No Schema Markup found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "61tll6yVWiu8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}